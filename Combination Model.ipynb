{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f933ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import traceback\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.config.config import Config, load_config_from_json\n",
    "from src.evaluate.rouge_evaluator import ScoreSummary\n",
    "from src.loader.class_loader import Dataset, SOURCE, Cluster\n",
    "from src.utils.factory import create_model, create_evaluator\n",
    "from src.model.sds.combination import CombinationRanker\n",
    "from src.loader.class_loader import load_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d2a371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cluster:  200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [04:39,  1.40s/it]\n",
      "WARNING:root:[PIPELINE] - Load train set from /home/dang/vlsp-final-year/dataset/vlsp_2022_abmusu_train_data_new.jsonl. Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cluster:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:10,  1.31s/it]\n",
      "WARNING:root:[PIPELINE] - Load valid set from /home/dang/vlsp-final-year/dataset/vlsp_2022_abmusu_validation_data_new.jsonl. Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cluster:  300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [05:55,  1.19s/it]\n",
      "WARNING:root:[PIPELINE] - Load test set from /home/dang/vlsp-final-year/dataset/vlsp_abmusu_test_data.jsonl. Done.\n"
     ]
    }
   ],
   "source": [
    "config = load_config_from_json()\n",
    "\n",
    "try:\n",
    "    train_set = load_cluster(\n",
    "        config.train_path,\n",
    "    )\n",
    "    logging.warning(\"[PIPELINE] - Load train set from {}. Done.\".format(config.train_path))\n",
    "except Exception as e:\n",
    "    train_set = None\n",
    "    logging.warning(\"[PIPELINE] - Load train set from {}. Failed. Using None.\".format(config.train_path))\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    valid_set = load_cluster(\n",
    "        config.valid_path,\n",
    "    )\n",
    "    logging.warning(\"[PIPELINE] - Load valid set from {}. Done.\".format(config.valid_path))\n",
    "except Exception as e:\n",
    "    valid_set = None\n",
    "    logging.warning(\"[PIPELINE] - Load valid set from {}. Failed. Using None.\".format(config.valid_path))\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    test_set = load_cluster(\n",
    "        \"/home/dang/vlsp-final-year/dataset/vlsp_abmusu_test_data.jsonl\",\n",
    "    )\n",
    "    logging.warning(\"[PIPELINE] - Load test set from {}. Done.\".format(\"/home/dang/vlsp-final-year/dataset/vlsp_abmusu_test_data.jsonl\"))\n",
    "except Exception as e:\n",
    "    test_set = None\n",
    "    logging.warning(\"[PIPELINE] - Load test set from {}. Failed. Using None.\".format(\"/home/dang/vlsp-final-year/dataset/vlsp_abmusu_test_data.jsonl\"))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789a3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "valid_set.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "test_set.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "\n",
    "train_scores = []\n",
    "valid_scores = []\n",
    "test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ea3baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:ComnbinationRanker-init: Start create a MMR Summarizer instance\n",
      "WARNING:root:TFIDF-init: Start create a MMR Summarizer instance\n",
      "WARNING:root:TFIDF-init: Model created\n",
      "WARNING:root:MMR-init: Start create a MMR Summarizer instance\n",
      "WARNING:root:Start create SBERT embedding\n",
      "Loading codes from /home/dang/vlsp-final-year/external/sentence_transformer/vn_sbert_deploy/bpe/bpe.codes ...\n",
      "Read 64000 codes from the codes file.\n",
      "WARNING:root:Create SBERT embedding complete\n",
      "WARNING:root:MMR-init: Model created\n",
      "WARNING:root:ComnbinationRanker-init: Model created\n"
     ]
    }
   ],
   "source": [
    "from src.model.sds.combination import CombinationRanker\n",
    "\n",
    "config = load_config_from_json()\n",
    "model_config = config.models[15]\n",
    "\n",
    "model = CombinationRanker(model_config)\n",
    "model.training(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31719922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get local score on train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:49<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get local score on valid set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:22<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get local score on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:04<00:00,  4.63it/s]\n"
     ]
    }
   ],
   "source": [
    "weight = {\n",
    "    \"tfidf\": 0.1,\n",
    "    \"lexrank\": 0.1,\n",
    "    \"textrank\": 0.1\n",
    "}\n",
    "    \n",
    "print(\"get local score on train set\")\n",
    "for cluster in tqdm(train_set.clusters):\n",
    "    document_score = []\n",
    "\n",
    "    for doc in cluster.documents:\n",
    "        document_score.append(model.get_score(doc.get_all_sents(), 1))\n",
    "\n",
    "    train_scores.append(document_score)\n",
    "\n",
    "print(\"get local score on valid set\")\n",
    "for cluster in tqdm(valid_set.clusters):\n",
    "    document_score = []\n",
    "\n",
    "    for doc in cluster.documents:\n",
    "        document_score.append(model.get_score(doc.get_all_sents(), 1))\n",
    "\n",
    "    valid_scores.append(document_score)\n",
    "    \n",
    "print(\"get local score on test set\")\n",
    "for cluster in tqdm(test_set.clusters):\n",
    "    document_score = []\n",
    "\n",
    "    for doc in cluster.documents:\n",
    "        document_score.append(model.get_score(doc.get_all_sents(), 1))\n",
    "\n",
    "    test_scores.append(document_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40922689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate.rouge_evaluator import RougeScore\n",
    "\n",
    "class RougeScoreStorage:\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame(columns=[\n",
    "            'cluster_id',\n",
    "            'rouge_1_p',\n",
    "            'rouge_1_r',\n",
    "            'rouge_1_f',\n",
    "            'rouge_2_p',\n",
    "            'rouge_2_r',\n",
    "            'rouge_2_f',\n",
    "            'rouge_l_p',\n",
    "            'rouge_l_r',\n",
    "            'rouge_l_f',\n",
    "        ])\n",
    "    \n",
    "    def add_score(self, cluster_id: int, score: RougeScore):\n",
    "        self.df = self.df.append({\n",
    "            'cluster_id': cluster_id,\n",
    "            'rouge_1_p': score.rouge1.p,\n",
    "            'rouge_1_r': score.rouge1.r,\n",
    "            'rouge_1_f': score.rouge1.f1,\n",
    "            'rouge_2_p': score.rouge2.p,\n",
    "            'rouge_2_r': score.rouge2.r,\n",
    "            'rouge_2_f': score.rouge2.f1,\n",
    "            'rouge_l_p': score.rougeL.p,\n",
    "            'rouge_l_r': score.rougeL.r,\n",
    "            'rouge_l_f': score.rougeL.f1,\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    def summary_score(self):\n",
    "        summary_df = pd.DataFrame(columns=[\n",
    "            'name',\n",
    "            'mean',\n",
    "            'min',\n",
    "            'max',\n",
    "            'std',\n",
    "        ])\n",
    "\n",
    "        metric_cols = [\n",
    "            'rouge_1_p',\n",
    "            'rouge_1_r',\n",
    "            'rouge_1_f',\n",
    "            'rouge_2_p',\n",
    "            'rouge_2_r',\n",
    "            'rouge_2_f',\n",
    "            'rouge_l_p',\n",
    "            'rouge_l_r',\n",
    "            'rouge_l_f', ]\n",
    "\n",
    "        for col in metric_cols:\n",
    "            describe = self.df[col].describe()\n",
    "            summary_df = summary_df.append({\n",
    "                'name': col,\n",
    "                'mean': describe['mean'],\n",
    "                'min': describe['min'],\n",
    "                'max': describe['max'],\n",
    "                'std': describe['std'],\n",
    "            }, ignore_index=True)\n",
    "        \n",
    "        return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684de60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate.rouge_evaluator import ScoreSummary\n",
    "from src.utils.factory import create_model, create_evaluator\n",
    "from src.model.mmr_query import MMRSummarizerQuery\n",
    "\n",
    "evaluator = create_evaluator(config.eval_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b62c798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sent = [11, 0.2]\n",
    "\n",
    "def get_rouge_score(weights, sigma):\n",
    "    local_config = config.models[15]\n",
    "    local_config.sigma = sigma\n",
    "    \n",
    "    mmr = MMRSummarizerQuery(local_config) \n",
    "    \n",
    "    train_storage = RougeScoreStorage()\n",
    "    valid_storage = RougeScoreStorage()\n",
    "    \n",
    "    print(\"Test on training set\")\n",
    "    \n",
    "    \"\"\"train_set.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "    for idx, cluster in tqdm(enumerate(train_set.clusters)):\n",
    "        chosen_sent = [] \n",
    "\n",
    "        sent_count = len(cluster.get_all_sents())\n",
    "        for SENT_COUNT in n_sent:\n",
    "            if 0 <= SENT_COUNT < 1:\n",
    "                sent_count = min(int(math.ceil(len(cluster.get_all_sents()) * SENT_COUNT)), sent_count)\n",
    "            else:\n",
    "                sent_count = min(int(SENT_COUNT), sent_count)\n",
    "\n",
    "\n",
    "        for idxc, doc in enumerate(cluster.documents):\n",
    "            doc.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "            sents = doc.get_all_sents()\n",
    "\n",
    "            scores = train_scores[idx][idxc]\n",
    "\n",
    "            combine_score = np.zeros((len(scores[\"tfidf\"])), dtype=float)\n",
    "\n",
    "            for key in weights.keys():\n",
    "                combine_score += scores[key] * weights[key]\n",
    "\n",
    "            if (sent_count >= len(combine_score)):\n",
    "                chosen_idx = list(range(len(combine_score)))\n",
    "            else:\n",
    "                chosen_idx = np.argpartition(combine_score, -sent_count)[-sent_count:]\n",
    "\n",
    "            for i in chosen_idx:\n",
    "                chosen_sent.append(sents[i]) \n",
    "\n",
    "        pred_sent, _ = mmr(chosen_sent, sent_count, cluster.get_all_anchor())\n",
    "\n",
    "        train_storage.add_score(\n",
    "            cluster.cluster_idx,\n",
    "            evaluator(\n",
    "                '.'.join(pred_sent),\n",
    "                '.'.join(cluster.get_summary()),\n",
    "            )\n",
    "        )\"\"\"\n",
    "    \n",
    "    print(\"Test on valid set\")\n",
    "    \n",
    "    valid_set.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "    for idx, cluster in tqdm(enumerate(valid_set.clusters)):\n",
    "        chosen_sent = [] \n",
    "        \n",
    "        sent_count = len(cluster.get_all_sents())\n",
    "        for SENT_COUNT in n_sent:\n",
    "            if 0 <= SENT_COUNT < 1:\n",
    "                sent_count = min(int(math.ceil(len(cluster.get_all_sents()) * SENT_COUNT)), sent_count)\n",
    "            else:\n",
    "                sent_count = min(int(SENT_COUNT), sent_count)\n",
    "        \n",
    "        \n",
    "        for idxc, doc in enumerate(cluster.documents):\n",
    "            doc.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "            sents = doc.get_all_sents()\n",
    "            \n",
    "            scores = valid_scores[idx][idxc]\n",
    "            \n",
    "            combine_score = np.zeros((len(scores[\"tfidf\"])), dtype=float)\n",
    "        \n",
    "            for key in weights.keys():\n",
    "                combine_score += scores[key] * weights[key]\n",
    "            \n",
    "            if (sent_count >= len(combine_score)):\n",
    "                chosen_idx = list(range(len(combine_score)))\n",
    "            else:\n",
    "                chosen_idx = np.argpartition(combine_score, -sent_count)[-sent_count:]\n",
    "            \n",
    "            for i in chosen_idx:\n",
    "                chosen_sent.append(sents[i]) \n",
    "                \n",
    "        pred_sent, _ = mmr(chosen_sent, sent_count, cluster.get_all_anchor())\n",
    "        \n",
    "        cluster.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "        valid_storage.add_score(\n",
    "            cluster.cluster_idx,\n",
    "            evaluator(\n",
    "                '.'.join(pred_sent),\n",
    "                '.'.join(cluster.get_summary()),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    print(\"Using weight\\n\", weights)\n",
    "    print(\"Sigma: \", sigma)\n",
    "    # print(\"Train result\\n\", train_storage.summary_score())\n",
    "    print(\"Valid result\\n\", valid_storage.summary_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b747eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_score_and_saved(weights, sigma):\n",
    "    local_config = config.models[15]\n",
    "    local_config.sigma = sigma\n",
    "    \n",
    "    mmr = MMRSummarizerQuery(local_config) \n",
    "    \n",
    "    train_storage = RougeScoreStorage()\n",
    "    valid_storage = RougeScoreStorage()\n",
    "    \n",
    "    print(\"Test on training set\")\n",
    "    \n",
    "    \"\"\"train_set.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "    for idx, cluster in tqdm(enumerate(train_set.clusters)):\n",
    "        chosen_sent = [] \n",
    "\n",
    "        sent_count = len(cluster.get_all_sents())\n",
    "        for SENT_COUNT in n_sent:\n",
    "            if 0 <= SENT_COUNT < 1:\n",
    "                sent_count = min(int(math.ceil(len(cluster.get_all_sents()) * SENT_COUNT)), sent_count)\n",
    "            else:\n",
    "                sent_count = min(int(SENT_COUNT), sent_count)\n",
    "\n",
    "\n",
    "        for idxc, doc in enumerate(cluster.documents):\n",
    "            doc.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "            sents = doc.get_all_sents()\n",
    "\n",
    "            scores = train_scores[idx][idxc]\n",
    "\n",
    "            combine_score = np.zeros((len(scores[\"tfidf\"])), dtype=float)\n",
    "\n",
    "            for key in weights.keys():\n",
    "                combine_score += scores[key] * weights[key]\n",
    "\n",
    "            if (sent_count >= len(combine_score)):\n",
    "                chosen_idx = list(range(len(combine_score)))\n",
    "            else:\n",
    "                chosen_idx = np.argpartition(combine_score, -sent_count)[-sent_count:]\n",
    "\n",
    "            for i in chosen_idx:\n",
    "                chosen_sent.append(sents[i]) \n",
    "\n",
    "        pred_sent, _ = mmr(chosen_sent, sent_count, cluster.get_all_anchor())\n",
    "\n",
    "        train_storage.add_score(\n",
    "            cluster.cluster_idx,\n",
    "            evaluator(\n",
    "                '.'.join(pred_sent),\n",
    "                '.'.join(cluster.get_summary()),\n",
    "            )\n",
    "        )\"\"\"\n",
    "    \n",
    "    print(\"Test on valid set\")\n",
    "    \n",
    "    valid_set.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "    for idx, cluster in tqdm(enumerate(valid_set.clusters)):\n",
    "        chosen_sent = [] \n",
    "        \n",
    "        sent_count = len(cluster.get_all_sents())\n",
    "        for SENT_COUNT in n_sent:\n",
    "            if 0 <= SENT_COUNT < 1:\n",
    "                sent_count = min(int(math.ceil(len(cluster.get_all_sents()) * SENT_COUNT)), sent_count)\n",
    "            else:\n",
    "                sent_count = min(int(SENT_COUNT), sent_count)\n",
    "        \n",
    "        \n",
    "        for idxc, doc in enumerate(cluster.documents):\n",
    "            doc.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "            sents = doc.get_all_sents()\n",
    "            \n",
    "            scores = valid_scores[idx][idxc]\n",
    "            \n",
    "            combine_score = np.zeros((len(scores[\"tfidf\"])), dtype=float)\n",
    "        \n",
    "            for key in weights.keys():\n",
    "                combine_score += scores[key] * weights[key]\n",
    "            \n",
    "            if (sent_count >= len(combine_score)):\n",
    "                chosen_idx = list(range(len(combine_score)))\n",
    "            else:\n",
    "                chosen_idx = np.argpartition(combine_score, -sent_count)[-sent_count:]\n",
    "            \n",
    "            for i in chosen_idx:\n",
    "                chosen_sent.append(sents[i]) \n",
    "                \n",
    "        pred_sent, _ = mmr(chosen_sent, sent_count, cluster.get_all_anchor())\n",
    "        \n",
    "        cluster.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "        valid_storage.add_score(\n",
    "            cluster.cluster_idx,\n",
    "            evaluator(\n",
    "                '.'.join(pred_sent),\n",
    "                '.'.join(cluster.get_summary()),\n",
    "            )\n",
    "        )\n",
    "     \n",
    "    print(\"Predict on test set\")\n",
    "    \n",
    "    test_set.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, cluster in tqdm(enumerate(test_set.clusters)):\n",
    "        chosen_sent = [] \n",
    "        \n",
    "        sent_count = len(cluster.get_all_sents())\n",
    "        for SENT_COUNT in n_sent:\n",
    "            if 0 <= SENT_COUNT < 1:\n",
    "                sent_count = min(int(math.ceil(len(cluster.get_all_sents()) * SENT_COUNT)), sent_count)\n",
    "            else:\n",
    "                sent_count = min(int(SENT_COUNT), sent_count)\n",
    "        \n",
    "        \n",
    "        for idxc, doc in enumerate(cluster.documents):\n",
    "            doc.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "            sents = doc.get_all_sents()\n",
    "            \n",
    "            scores = test_scores[idx][idxc]\n",
    "            \n",
    "            combine_score = np.zeros((len(scores[\"tfidf\"])), dtype=float)\n",
    "        \n",
    "            for key in weights.keys():\n",
    "                combine_score += scores[key] * weights[key]\n",
    "            \n",
    "            if (sent_count >= len(combine_score)):\n",
    "                chosen_idx = list(range(len(combine_score)))\n",
    "            else:\n",
    "                chosen_idx = np.argpartition(combine_score, -sent_count)[-sent_count:]\n",
    "            \n",
    "            for i in chosen_idx:\n",
    "                chosen_sent.append(sents[i]) \n",
    "                   \n",
    "        pred_sent, _ = mmr(chosen_sent, sent_count, cluster.get_all_anchor())\n",
    "        predictions.append(' '.join(pred_sent))\n",
    "        \n",
    "    print(\"Start write to txt\")\n",
    "    with open(os.path.join(\"/home/dang/vlsp-final-year/data/result/outer/combination\", \"results.txt\"), \"w\") as f:\n",
    "        for summary in predictions:\n",
    "            summary.replace('_', ' ')\n",
    "            f.write(summary)\n",
    "            f.write('\\n')\n",
    "    print(\"Done write to txt\")\n",
    "                \n",
    "    print(\"Using weight\\n\", weights)\n",
    "    print(\"Sigma: \", sigma)\n",
    "    #print(\"Train result\\n\", train_storage.summary_score())\n",
    "    print(\"Valid result\\n\", valid_storage.summary_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2c109cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for sigma in [0.0, 0.2, 0.8, 1.0]:\n",
    "#     for i in [0.0, 0.3, 0.7, 1.0]:\n",
    "#         for j in [0.0, 0.3, 0.7, 1.0]:\n",
    "#             if i + j > 1.0:\n",
    "#                 continue \n",
    "#             k = 1.0 - i - j \n",
    "\n",
    "#             weight = {\n",
    "#                 \"tfidf\": i,\n",
    "#                 \"lexrank\": j,\n",
    "#                 \"textrank\": k\n",
    "#             }\n",
    "\n",
    "#             get_rouge_score(weight, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b572a130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:MMR-init: Start create a MMR Summarizer instance\n",
      "WARNING:root:Start create SBERT embedding\n",
      "Loading codes from /home/dang/vlsp-final-year/external/sentence_transformer/vn_sbert_deploy/bpe/bpe.codes ...\n",
      "Read 64000 codes from the codes file.\n",
      "WARNING:root:Create SBERT embedding complete\n",
      "WARNING:root:MMR-init: Model created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on training set\n",
      "Test on valid set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [03:38,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [07:41,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start write to txt\n",
      "Done write to txt\n",
      "Using weight\n",
      " {'tfidf': 0.0, 'lexrank': 0.8, 'textrank': 0.2}\n",
      "Sigma:  0.8\n",
      "Valid result\n",
      "         name      mean       min       max       std\n",
      "0  rouge_1_p  0.428603  0.195266  1.000000  0.144988\n",
      "1  rouge_1_r  0.609957  0.353535  1.000000  0.129503\n",
      "2  rouge_1_f  0.493109  0.285714  1.000000  0.123463\n",
      "3  rouge_2_p  0.247248  0.020000  0.967742  0.159538\n",
      "4  rouge_2_r  0.384883  0.041667  0.967742  0.178236\n",
      "5  rouge_2_f  0.292535  0.027460  0.967742  0.158970\n",
      "6  rouge_l_p  0.405636  0.184466  1.000000  0.148301\n",
      "7  rouge_l_r  0.576432  0.323232  1.000000  0.138975\n",
      "8  rouge_l_f  0.466368  0.241270  1.000000  0.131014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weight = {\n",
    "    \"tfidf\": 0.0,\n",
    "    \"lexrank\": 0.8,\n",
    "    \"textrank\": 0.2\n",
    "}\n",
    "\n",
    "get_rouge_score_and_saved(weight, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5288c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
