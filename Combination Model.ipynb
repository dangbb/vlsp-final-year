{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f933ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import traceback\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.config.config import Config, load_config_from_json\n",
    "from src.evaluate.rouge_evaluator import ScoreSummary\n",
    "from src.loader.class_loader import Dataset, SOURCE, Cluster\n",
    "from src.utils.factory import create_model, create_evaluator\n",
    "from src.model.sds.combination import CombinationRanker\n",
    "from src.loader.class_loader import load_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d2a371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cluster:  200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:38,  5.23it/s]\n",
      "WARNING:root:[PIPELINE] - Load train set from /home/dang/vlsp-final-year/dataset/vlsp_2022_abmusu_train_data_new.jsonl. Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cluster:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:18,  5.36it/s]\n",
      "WARNING:root:[PIPELINE] - Load valid set from /home/dang/vlsp-final-year/dataset/vlsp_2022_abmusu_validation_data_new.jsonl. Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cluster:  300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:49,  6.09it/s]\n",
      "WARNING:root:[PIPELINE] - Load test set from /home/dang/vlsp-final-year/dataset/vlsp_abmusu_test_data.jsonl. Done.\n"
     ]
    }
   ],
   "source": [
    "config = load_config_from_json()\n",
    "\n",
    "try:\n",
    "    train_set = load_cluster(\n",
    "        config.train_path,\n",
    "    )\n",
    "    logging.warning(\"[PIPELINE] - Load train set from {}. Done.\".format(config.train_path))\n",
    "except Exception as e:\n",
    "    train_set = None\n",
    "    logging.warning(\"[PIPELINE] - Load train set from {}. Failed. Using None.\".format(config.train_path))\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    valid_set = load_cluster(\n",
    "        config.valid_path,\n",
    "    )\n",
    "    logging.warning(\"[PIPELINE] - Load valid set from {}. Done.\".format(config.valid_path))\n",
    "except Exception as e:\n",
    "    valid_set = None\n",
    "    logging.warning(\"[PIPELINE] - Load valid set from {}. Failed. Using None.\".format(config.valid_path))\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    test_set = load_cluster(\n",
    "        \"/home/dang/vlsp-final-year/dataset/vlsp_abmusu_test_data.jsonl\",\n",
    "    )\n",
    "    logging.warning(\"[PIPELINE] - Load test set from {}. Done.\".format(\"/home/dang/vlsp-final-year/dataset/vlsp_abmusu_test_data.jsonl\"))\n",
    "except Exception as e:\n",
    "    test_set = None\n",
    "    logging.warning(\"[PIPELINE] - Load test set from {}. Failed. Using None.\".format(\"/home/dang/vlsp-final-year/dataset/vlsp_abmusu_test_data.jsonl\"))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789a3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "valid_set.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "test_set.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "\n",
    "train_scores = []\n",
    "valid_scores = []\n",
    "test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ea3baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:ComnbinationRanker-init: Start create a MMR Summarizer instance\n",
      "WARNING:root:TFIDF-init: Start create a MMR Summarizer instance\n",
      "WARNING:root:TFIDF-init: Model created\n",
      "WARNING:root:MMR-init: Start create a MMR Summarizer instance\n",
      "WARNING:root:Start create SBERT embedding\n",
      "Loading codes from /home/dang/vlsp-final-year/external/sentence_transformer/vn_sbert_deploy/bpe/bpe.codes ...\n",
      "Read 64000 codes from the codes file.\n",
      "WARNING:root:Create SBERT embedding complete\n",
      "WARNING:root:MMR-init: Model created\n",
      "WARNING:root:ComnbinationRanker-init: Model created\n"
     ]
    }
   ],
   "source": [
    "from src.model.sds.combination import CombinationRanker\n",
    "\n",
    "config = load_config_from_json()\n",
    "model_config = config.models[15]\n",
    "\n",
    "model = CombinationRanker(model_config)\n",
    "model.training(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31719922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get local score on train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:43<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get local score on valid set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get local score on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:59<00:00,  5.06it/s]\n"
     ]
    }
   ],
   "source": [
    "weight = {\n",
    "    \"tfidf\": 0.1,\n",
    "    \"lexrank\": 0.1,\n",
    "    \"textrank\": 0.1\n",
    "}\n",
    "    \n",
    "print(\"get local score on train set\")\n",
    "for cluster in tqdm(train_set.clusters):\n",
    "    document_score = []\n",
    "\n",
    "    for doc in cluster.documents:\n",
    "        document_score.append(model.get_score(doc.get_all_sents(), 1))\n",
    "\n",
    "    train_scores.append(document_score)\n",
    "\n",
    "print(\"get local score on valid set\")\n",
    "for cluster in tqdm(valid_set.clusters):\n",
    "    document_score = []\n",
    "\n",
    "    for doc in cluster.documents:\n",
    "        document_score.append(model.get_score(doc.get_all_sents(), 1))\n",
    "\n",
    "    valid_scores.append(document_score)\n",
    "    \n",
    "print(\"get local score on test set\")\n",
    "for cluster in tqdm(test_set.clusters):\n",
    "    document_score = []\n",
    "\n",
    "    for doc in cluster.documents:\n",
    "        document_score.append(model.get_score(doc.get_all_sents(), 1))\n",
    "\n",
    "    test_scores.append(document_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40922689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate.rouge_evaluator import RougeScore\n",
    "\n",
    "class RougeScoreStorage:\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame(columns=[\n",
    "            'cluster_id',\n",
    "            'rouge_1_p',\n",
    "            'rouge_1_r',\n",
    "            'rouge_1_f',\n",
    "            'rouge_2_p',\n",
    "            'rouge_2_r',\n",
    "            'rouge_2_f',\n",
    "            'rouge_l_p',\n",
    "            'rouge_l_r',\n",
    "            'rouge_l_f',\n",
    "        ])\n",
    "    \n",
    "    def add_score(self, cluster_id: int, score: RougeScore):\n",
    "        self.df = self.df.append({\n",
    "            'cluster_id': cluster_id,\n",
    "            'rouge_1_p': score.rouge1.p,\n",
    "            'rouge_1_r': score.rouge1.r,\n",
    "            'rouge_1_f': score.rouge1.f1,\n",
    "            'rouge_2_p': score.rouge2.p,\n",
    "            'rouge_2_r': score.rouge2.r,\n",
    "            'rouge_2_f': score.rouge2.f1,\n",
    "            'rouge_l_p': score.rougeL.p,\n",
    "            'rouge_l_r': score.rougeL.r,\n",
    "            'rouge_l_f': score.rougeL.f1,\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    def summary_score(self):\n",
    "        summary_df = pd.DataFrame(columns=[\n",
    "            'name',\n",
    "            'mean',\n",
    "            'min',\n",
    "            'max',\n",
    "            'std',\n",
    "        ])\n",
    "\n",
    "        metric_cols = [\n",
    "            'rouge_1_p',\n",
    "            'rouge_1_r',\n",
    "            'rouge_1_f',\n",
    "            'rouge_2_p',\n",
    "            'rouge_2_r',\n",
    "            'rouge_2_f',\n",
    "            'rouge_l_p',\n",
    "            'rouge_l_r',\n",
    "            'rouge_l_f', ]\n",
    "\n",
    "        for col in metric_cols:\n",
    "            describe = self.df[col].describe()\n",
    "            summary_df = summary_df.append({\n",
    "                'name': col,\n",
    "                'mean': describe['mean'],\n",
    "                'min': describe['min'],\n",
    "                'max': describe['max'],\n",
    "                'std': describe['std'],\n",
    "            }, ignore_index=True)\n",
    "        \n",
    "        return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684de60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:MMR-init: Start create a MMR Summarizer instance\n",
      "WARNING:root:Start create SBERT embedding\n",
      "Loading codes from /home/dang/vlsp-final-year/external/sentence_transformer/vn_sbert_deploy/bpe/bpe.codes ...\n",
      "Read 64000 codes from the codes file.\n",
      "WARNING:root:Create SBERT embedding complete\n",
      "WARNING:root:MMR-init: Model created\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate.rouge_evaluator import ScoreSummary\n",
    "from src.utils.factory import create_model, create_evaluator\n",
    "from src.model.mmr_query import MMRSummarizerQuery\n",
    "\n",
    "evaluator = create_evaluator(config.eval_config)\n",
    "\n",
    "mmr = MMRSummarizerQuery(config.models[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b62c798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sent = [11, 0.2]\n",
    "\n",
    "def get_rouge_score(weights):\n",
    "    train_storage = RougeScoreStorage()\n",
    "    valid_storage = RougeScoreStorage()\n",
    "    \n",
    "    print(\"Test on training set\")\n",
    "    for idx, cluster in tqdm(enumerate(train_set.clusters)):\n",
    "        chosen_sent = [] \n",
    "        \n",
    "        sent_count = len(cluster.get_all_sents())\n",
    "        for SENT_COUNT in n_sent:\n",
    "            if 0 <= SENT_COUNT < 1:\n",
    "                sent_count = min(int(math.ceil(len(cluster.get_all_sents()) * SENT_COUNT)), sent_count)\n",
    "            else:\n",
    "                sent_count = min(int(SENT_COUNT), sent_count)\n",
    "        \n",
    "        \n",
    "        for idxc, doc in enumerate(cluster.documents):\n",
    "            sents = doc.get_all_sents()\n",
    "            \n",
    "            scores = train_scores[idx][idxc]\n",
    "            \n",
    "            combine_score = np.zeros((len(scores[\"tfidf\"])), dtype=float)\n",
    "        \n",
    "            for key in weights.keys():\n",
    "                combine_score += scores[key] * weights[key]\n",
    "            \n",
    "            if (sent_count >= len(combine_score)):\n",
    "                chosen_idx = list(range(len(combine_score)))\n",
    "            else:\n",
    "                chosen_idx = np.argpartition(combine_score, -sent_count)[-sent_count:]\n",
    "            \n",
    "            for i in chosen_idx:\n",
    "                chosen_sent.append(sents[i]) \n",
    "                \n",
    "        pred_sent, _ = mmr(chosen_sent, sent_count, cluster.get_all_anchor())\n",
    "                \n",
    "        train_storage.add_score(\n",
    "            cluster.cluster_idx,\n",
    "            evaluator(\n",
    "                '.'.join(pred_sent),\n",
    "                '.'.join(cluster.get_summary()),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    print(\"Test on valid set\")\n",
    "    for idx, cluster in tqdm(enumerate(valid_set.clusters)):\n",
    "        chosen_sent = [] \n",
    "        \n",
    "        sent_count = len(cluster.get_all_sents())\n",
    "        for SENT_COUNT in n_sent:\n",
    "            if 0 <= SENT_COUNT < 1:\n",
    "                sent_count = min(int(math.ceil(len(cluster.get_all_sents()) * SENT_COUNT)), sent_count)\n",
    "            else:\n",
    "                sent_count = min(int(SENT_COUNT), sent_count)\n",
    "        \n",
    "        \n",
    "        for idxc, doc in enumerate(cluster.documents):\n",
    "            sents = doc.get_all_sents()\n",
    "            \n",
    "            scores = valid_scores[idx][idxc]\n",
    "            \n",
    "            combine_score = np.zeros((len(scores[\"tfidf\"])), dtype=float)\n",
    "        \n",
    "            for key in weights.keys():\n",
    "                combine_score += scores[key] * weights[key]\n",
    "            \n",
    "            if (sent_count >= len(combine_score)):\n",
    "                chosen_idx = list(range(len(combine_score)))\n",
    "            else:\n",
    "                chosen_idx = np.argpartition(combine_score, -sent_count)[-sent_count:]\n",
    "            \n",
    "            for i in chosen_idx:\n",
    "                chosen_sent.append(sents[i]) \n",
    "                \n",
    "        pred_sent, _ = mmr(chosen_sent, sent_count, cluster.get_all_anchor())\n",
    "                \n",
    "        valid_storage.add_score(\n",
    "            cluster.cluster_idx,\n",
    "            evaluator(\n",
    "                '.'.join(pred_sent),\n",
    "                '.'.join(cluster.get_summary()),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    print(\"Using weight\\n\", weights)\n",
    "    print(\"Train result\\n\", train_storage.summary_score())\n",
    "    print(\"Valid result\\n\", valid_storage.summary_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b747eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_score_and_saved(weights):\n",
    "    train_storage = RougeScoreStorage()\n",
    "    valid_storage = RougeScoreStorage()\n",
    "    \n",
    "    print(\"Test on training set\")\n",
    "    \n",
    "    \"\"\"train_set.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "    for idx, cluster in tqdm(enumerate(train_set.clusters)):\n",
    "        chosen_sent = [] \n",
    "\n",
    "        sent_count = len(cluster.get_all_sents())\n",
    "        for SENT_COUNT in n_sent:\n",
    "            if 0 <= SENT_COUNT < 1:\n",
    "                sent_count = min(int(math.ceil(len(cluster.get_all_sents()) * SENT_COUNT)), sent_count)\n",
    "            else:\n",
    "                sent_count = min(int(SENT_COUNT), sent_count)\n",
    "\n",
    "\n",
    "        for idxc, doc in enumerate(cluster.documents):\n",
    "            doc.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "            sents = doc.get_all_sents()\n",
    "\n",
    "            scores = train_scores[idx][idxc]\n",
    "\n",
    "            combine_score = np.zeros((len(scores[\"tfidf\"])), dtype=float)\n",
    "\n",
    "            for key in weights.keys():\n",
    "                combine_score += scores[key] * weights[key]\n",
    "\n",
    "            if (sent_count >= len(combine_score)):\n",
    "                chosen_idx = list(range(len(combine_score)))\n",
    "            else:\n",
    "                chosen_idx = np.argpartition(combine_score, -sent_count)[-sent_count:]\n",
    "\n",
    "            for i in chosen_idx:\n",
    "                chosen_sent.append(sents[i]) \n",
    "\n",
    "        pred_sent, _ = mmr(chosen_sent, sent_count, cluster.get_all_anchor())\n",
    "\n",
    "        train_storage.add_score(\n",
    "            cluster.cluster_idx,\n",
    "            evaluator(\n",
    "                '.'.join(pred_sent),\n",
    "                '.'.join(cluster.get_summary()),\n",
    "            )\n",
    "        )\"\"\"\n",
    "    \n",
    "    print(\"Test on valid set\")\n",
    "    \n",
    "    valid_set.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "    for idx, cluster in tqdm(enumerate(valid_set.clusters)):\n",
    "        chosen_sent = [] \n",
    "        \n",
    "        sent_count = len(cluster.get_all_sents())\n",
    "        for SENT_COUNT in n_sent:\n",
    "            if 0 <= SENT_COUNT < 1:\n",
    "                sent_count = min(int(math.ceil(len(cluster.get_all_sents()) * SENT_COUNT)), sent_count)\n",
    "            else:\n",
    "                sent_count = min(int(SENT_COUNT), sent_count)\n",
    "        \n",
    "        \n",
    "        for idxc, doc in enumerate(cluster.documents):\n",
    "            doc.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "            sents = doc.get_all_sents()\n",
    "            \n",
    "            scores = valid_scores[idx][idxc]\n",
    "            \n",
    "            combine_score = np.zeros((len(scores[\"tfidf\"])), dtype=float)\n",
    "        \n",
    "            for key in weights.keys():\n",
    "                combine_score += scores[key] * weights[key]\n",
    "            \n",
    "            if (sent_count >= len(combine_score)):\n",
    "                chosen_idx = list(range(len(combine_score)))\n",
    "            else:\n",
    "                chosen_idx = np.argpartition(combine_score, -sent_count)[-sent_count:]\n",
    "            \n",
    "            for i in chosen_idx:\n",
    "                chosen_sent.append(sents[i]) \n",
    "                \n",
    "        pred_sent, _ = mmr(chosen_sent, sent_count, cluster.get_all_anchor())\n",
    "        \n",
    "        cluster.set_source(SOURCE.SENT_SPLITTED_TOKEN.value)\n",
    "        valid_storage.add_score(\n",
    "            cluster.cluster_idx,\n",
    "            evaluator(\n",
    "                '.'.join(pred_sent),\n",
    "                '.'.join(cluster.get_summary()),\n",
    "            )\n",
    "        )\n",
    "        if idx == 0:\n",
    "            print(\"Predict:\\n\", '.'.join(pred_sent))\n",
    "            print(\"Golden:\\n\", '.'.join(cluster.get_summary()))\n",
    "     \n",
    "    print(\"Predict on test set\")\n",
    "    \n",
    "    test_set.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, cluster in tqdm(enumerate(test_set.clusters)):\n",
    "        chosen_sent = [] \n",
    "        \n",
    "        sent_count = len(cluster.get_all_sents())\n",
    "        for SENT_COUNT in n_sent:\n",
    "            if 0 <= SENT_COUNT < 1:\n",
    "                sent_count = min(int(math.ceil(len(cluster.get_all_sents()) * SENT_COUNT)), sent_count)\n",
    "            else:\n",
    "                sent_count = min(int(SENT_COUNT), sent_count)\n",
    "        \n",
    "        \n",
    "        for idxc, doc in enumerate(cluster.documents):\n",
    "            doc.set_source(SOURCE.SENT_SPLITTED_TEXT.value)\n",
    "            sents = doc.get_all_sents()\n",
    "            \n",
    "            scores = test_scores[idx][idxc]\n",
    "            \n",
    "            combine_score = np.zeros((len(scores[\"tfidf\"])), dtype=float)\n",
    "        \n",
    "            for key in weights.keys():\n",
    "                combine_score += scores[key] * weights[key]\n",
    "            \n",
    "            if (sent_count >= len(combine_score)):\n",
    "                chosen_idx = list(range(len(combine_score)))\n",
    "            else:\n",
    "                chosen_idx = np.argpartition(combine_score, -sent_count)[-sent_count:]\n",
    "            \n",
    "            for i in chosen_idx:\n",
    "                chosen_sent.append(sents[i]) \n",
    "                   \n",
    "        pred_sent, _ = mmr(chosen_sent, sent_count, cluster.get_all_anchor())\n",
    "        predictions.append(' '.join(pred_sent))\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(\"Predict:\\n\", ' '.join(pred_sent))\n",
    "        \n",
    "    print(\"Start write to txt\")\n",
    "    with open(os.path.join(\"/home/dang/vlsp-final-year/data/result/outer/combination\", \"results.txt\"), \"w\") as f:\n",
    "        for summary in predictions:\n",
    "            summary.replace('_', ' ')\n",
    "            f.write(summary)\n",
    "            f.write('\\n')\n",
    "    print(\"Done write to txt\")\n",
    "                \n",
    "        \n",
    "        \n",
    "    print(\"Using weight\\n\", weights)\n",
    "    #print(\"Train result\\n\", train_storage.summary_score())\n",
    "    print(\"Valid result\\n\", valid_storage.summary_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c109cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [idx * .2 for idx in range(5)]:\n",
    "#     for j in [idx * .2 for idx in range(5)]:\n",
    "#         if i + j > 1.0:\n",
    "#             continue \n",
    "#         k = 1.0 - i - j \n",
    "        \n",
    "#         weight = {\n",
    "#             \"tfidf\": i,\n",
    "#             \"lexrank\": j,\n",
    "#             \"textrank\": k\n",
    "#         }\n",
    "        \n",
    "#         get_rouge_score(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b572a130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on training set\n",
      "Test on valid set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict:\n",
      " Ngày 22/5 một chuyến bay khởi_hành từ Villahermosa và hướng đến Mexico_City thì một chú chim bất_ngờ lao vào tuabin động_cơ khiến máy_bay phải hạ_cánh khẩn_cấp may_mắn không xảy ra tai_nạn đáng tiếc nào.Theo RT vài phút sau khi chiếc máy_bay Airbus_A320 cất_cánh lúc 22 h ngày 23/8 một tiếng nổ lớn đã vang lên và lửa bắt_đầu phụt ra từ động_cơ bên phải.Phi_hành_đoàn nhận được cảnh_báo khi máy_bay đang ở độ cao tương_đương 4.000 m và đã quay trở_lại hạ_cánh tại sân_bay Guadalajara 45 phút sau khi cất_cánh.Vụ cháy khiến các hành_khách hoảng_loạn khóc_lóc la_hét và cầu_nguyện\n",
      "Golden:\n",
      " Chuyến bay của hãng Viva_Aerobus mang số_hiệu VB518 có sức chứa 186 hành_khách khởi_hành từ Guadalajara Mexico tối 24/8 giờ_địa_phương dự_kiến kéo_dài 3 tiếng.Khoảng 10 phút sau khi cất_cánh hành_khách phát_hiện tia lửa bắn ra từ động_cơ bên phải của máy_bay.45 phút sau khi cất_cánh khi máy_bay đang ở độ cao tương_đương 4.000 m phi_hành_đoàn đã cho máy_bay quay trở_lại hạ_cánh tại sân_bay Guadalajara.Rất may_mắn vụ_việc này không biến thành một thảm_họa hàng_không.Nguyên_nhân vụ_việc đang tiếp_tục được điều_tra làm rõ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [04:15,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict:\n",
      " Những ngôi mộ cổ được tìm thấy bởi công ty khảo cổ ArchaeoTask GmbH, trong quá trình khảo sát một mảnh đất gần bờ sông Danube để đào một chiếc ao lớn chứa nước mưa. Với niên đại và giá trị lịch sử đó, các phát hiện tại Geisingen-Gutmadingen - bao gồm một kho hiện vật cực kỳ phong phú - là một kho tàng khảo cổ vĩ đại. Khi địa điểm này được mở từng cái một, người ta phát hiện ra đây là địa điểm Lajia đã biến mất trong trận động đất 4000 năm trước. Cạnh đó là 140 ngôi mộ đầu thời Trung Cổ, có niên đại từ năm 500 đến 600 sau Công Nguyên, chứa hàng hóa bao gồm kiếm, thương, khiên, lược xương, ly uống nước và hoa tai. Những ngôi mộ thời Trung Cổ thì thuộc về thời kỳ mà lãnh chúa Đức Odoacer phế truất hoàng đế La Mã Romulus Augustus, một phần của \"Thời kỳ di cư\" cực kỳ quan trọng ở châu Âu, đánh dấu sự di chuyển của các bộ tộc, cách họ chinh phục lẫn nhau, đẩy nhau sang các lãnh thổ mới... từ đó dần định hình nên châu Âu hiện đại. Tại một địa điểm thuộc quận Geisingen-Gutmadingen của Tuttlingen phía Tây Nam nước Đức, các nhà khảo cổ đã phát hiện ra một ngôi mộ từ thời kỳ đồ đá mới, có niên đại vào thiên niên kỷ thứ ba trước Công Nguyên.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [15:11,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start write to txt\n",
      "Done write to txt\n",
      "Using weight\n",
      " {'tfidf': 0.0, 'lexrank': 0.8, 'textrank': 0.2}\n",
      "Valid result\n",
      "         name      mean       min       max       std\n",
      "0  rouge_1_p  0.414330  0.196970  0.777778  0.136481\n",
      "1  rouge_1_r  0.611160  0.200000  0.986667  0.129249\n",
      "2  rouge_1_f  0.482777  0.267123  0.865497  0.120605\n",
      "3  rouge_2_p  0.238921  0.035556  0.736842  0.149251\n",
      "4  rouge_2_r  0.385006  0.041667  0.947368  0.170378\n",
      "5  rouge_2_f  0.286513  0.046110  0.743802  0.153401\n",
      "6  rouge_l_p  0.390106  0.171717  0.777778  0.142303\n",
      "7  rouge_l_r  0.574309  0.127273  0.986667  0.138422\n",
      "8  rouge_l_f  0.454431  0.172840  0.865497  0.130666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weight = {\n",
    "    \"tfidf\": 0.0,\n",
    "    \"lexrank\": 0.8,\n",
    "    \"textrank\": 0.2\n",
    "}\n",
    "\n",
    "get_rouge_score_and_saved(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2357e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
